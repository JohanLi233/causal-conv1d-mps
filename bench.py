import time
import argparse
import torch
import torch.nn.functional as F
import numpy as np
import causal_conv1d_mps as ccmps


# å…¨å±€ dtype ä¸å®¹å·®ï¼ˆé»˜è®¤ä½¿ç”¨ bf16ï¼‰
DTYPE = torch.bfloat16
ATOL_BY_DTYPE = {
    torch.float32: 1e-4,
    torch.float16: 5e-3,
    torch.bfloat16: 1e-2,
}


def get_tolerance_for(dtype: torch.dtype) -> float:
    return ATOL_BY_DTYPE.get(dtype, 1e-4)


def bench_robust(fn, warmup=10, iters=50, runs=5):
    """æ›´ç¨³å®šçš„æ€§èƒ½æµ‹è¯•å‡½æ•°"""
    results = []

    for run in range(runs):
        # æ¯æ¬¡è¿è¡Œå‰éƒ½é¢„çƒ­
        for _ in range(warmup):
            fn()
        torch.mps.synchronize()

        # æµ‹é‡å¤šæ¬¡è¿­ä»£
        times = []
        for _ in range(iters):
            t0 = time.time()
            fn()
            torch.mps.synchronize()
            t1 = time.time()
            times.append(t1 - t0)

        # å»æ‰æœ€é«˜å’Œæœ€ä½å€¼ï¼Œè®¡ç®—å¹³å‡å€¼
        times = sorted(times)[2:-2]  # å»æ‰å‰åå„2ä¸ªæå€¼
        avg_time = sum(times) / len(times)
        results.append(avg_time)

    # å»æ‰æœ€é«˜å’Œæœ€ä½çš„è¿è¡Œï¼Œè¿”å›ä¸­ä½æ•°
    results = sorted(results)[1:-1]
    return sum(results) / len(results)


def bench_robust_stable(fn, warmup=25, iters=100, runs=5, desc=""):
    """
    ä¸€ä¸ªæ›´ç¨³å®šã€æ›´å¥å£®çš„æ€§èƒ½æµ‹è¯•å‡½æ•°ã€‚

    ä¸»è¦æ”¹è¿›:
    1. åœ¨æ‰€æœ‰è¿è¡Œ(runs)å¼€å§‹å‰è¿›è¡Œä¸€æ¬¡å……åˆ†çš„é¢„çƒ­ã€‚
    2. æ¯æ¬¡æµ‹é‡éƒ½ä¸¥æ ¼ä½¿ç”¨ torch.mps.synchronize() åŒ…è£¹ã€‚
    3. è¿”å›å¤šæ¬¡è¿è¡Œ(runs)çš„ã€ä¸­ä½æ•°ã€‘æ—¶é—´ï¼Œå®ƒå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿã€‚
    4. åŒæ—¶è¿”å›æ ‡å‡†å·®ï¼Œç”¨äºè¯„ä¼°ç»“æœçš„ç¨³å®šæ€§ã€‚
    """
    # é›†ä¸­é¢„çƒ­ï¼šåœ¨æ‰€æœ‰è®¡æ—¶å¼€å§‹å‰ï¼Œè®©GPUè¾¾åˆ°ç¨³å®šå·¥ä½œçŠ¶æ€
    if desc:
        print(f"{desc:<20} {'é¢„çƒ­ä¸­...':<10}", end="\r")
    for _ in range(warmup):
        fn()
    torch.mps.synchronize()

    run_times = []
    for _ in range(runs):
        # æ¯æ¬¡è¿è¡Œéƒ½ç‹¬ç«‹è®¡æ—¶ï¼Œæ›´èƒ½æŠµæŠ—ç³»ç»Ÿå¹²æ‰°
        torch.mps.synchronize()
        t0 = time.time()
        for _ in range(iters):
            fn()
        torch.mps.synchronize()
        t1 = time.time()

        # è®¡ç®—å•æ¬¡è¿­ä»£çš„å¹³å‡æ—¶é—´
        avg_iter_time = (t1 - t0) / iters
        run_times.append(avg_iter_time)

    # ç»Ÿè®¡åˆ†æï¼šè®¡ç®—ä¸­ä½æ•°å’Œæ ‡å‡†å·®
    median_time = np.median(run_times)
    std_dev = np.std(run_times)

    return median_time, std_dev


def causal_conv1d_reference(x, weight, bias=None, silu_activation=False):
    """PyTorch å‚è€ƒå®ç°"""
    batch, dim, seqlen = x.shape
    width = weight.shape[1]

    # ä½¿ç”¨ F.conv1d å®ç°å› æœå·ç§¯
    x_padded = F.pad(x, (width - 1, 0))  # å·¦ä¾§å¡«å……

    out = F.conv1d(x_padded, weight.unsqueeze(1), bias=bias, groups=dim, padding=0)
    out = out[:, :, :seqlen]  # æˆªå–åˆ°åŸå§‹é•¿åº¦

    if silu_activation:
        out = F.silu(out)

    return out


def canon_forward_reference(x_btd, weight_dw, bias_d=None, activation: bool = True):
    """
    å‚è€ƒç‰ˆ Canon å‰å‘ï¼šè¾“å…¥ [B, T, D]ï¼Œä¸ lingua çš„ Canon ä¸€è‡´ã€‚
    - depthwise ç»„å·ç§¯ (groups=D)ï¼Œkernel æƒé‡å½¢çŠ¶ä¸º [D, W]
    - å› æœå¡«å…… padd_left=W-1
    - å¯é€‰ SiLU æ¿€æ´»
    è¿”å›åŒå½¢çŠ¶ [B, T, D]
    """
    b, t, d = x_btd.shape
    w = weight_dw.shape[1]
    x_bdt = x_btd.movedim(-1, -2)  # [B, D, T]
    x_pad = F.pad(x_bdt, (w - 1, 0))
    y = F.conv1d(x_pad, weight_dw.unsqueeze(1), bias=bias_d, groups=d)
    y = y[..., :t]
    if activation:
        y = F.silu(y)
    return y.movedim(-2, -1)


def generate_attention_mask(batch: int, seqlen: int, device: torch.device, min_fill_ratio: float = 0.5):
    """
    ç”Ÿæˆ HuggingFace é£æ ¼çš„ 2D attention_maskï¼ˆ1 è¡¨ç¤ºæœ‰æ•ˆï¼Œ0 è¡¨ç¤º paddingï¼‰ã€‚
    æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆé•¿åº¦åœ¨ [min_fill_ratio*seqlen, seqlen] ä¹‹é—´éšæœºã€‚
    è¿”å›: mask (B, T) - float32
    """
    min_len = max(1, int(seqlen * min_fill_ratio))
    lengths = torch.randint(low=min_len, high=seqlen + 1, size=(batch,), device=device)
    mask = torch.zeros(batch, seqlen, device=device, dtype=torch.float32)
    for b in range(batch):
        mask[b, : lengths[b].item()] = 1.0
    return mask


def short_conv_hf_reference(
    x_btd: torch.Tensor,
    weight_dw: torch.Tensor,
    bias_d: torch.Tensor | None,
    activation: bool = True,
    residual: bool = True,
    attention_mask: torch.Tensor | None = None,
):
    """
    æ¨¡æ‹Ÿ HuggingFace `ShortConvolution.forward` çš„å…³é”®è·¯å¾„ï¼š
    - è¾“å…¥ä¸º [B, T, D]
    - å…ˆæŒ‰ mask å°† padding ä½ç½®ç½®é›¶
    - åš depthwise causal convï¼ˆå¯é€‰ SiLUï¼‰
    - å¯é€‰ residualï¼ˆå°†ç»“æœä¸åŸå§‹ x ç›¸åŠ ï¼‰
    è¿”å› [B, T, D]
    """
    x_in = x_btd
    if attention_mask is not None:
        m = attention_mask.to(dtype=x_btd.dtype)
        x_btd = x_btd * m.unsqueeze(-1)
    y = canon_forward_reference(x_btd, weight_dw, bias_d, activation)
    if residual:
        y = x_in + y
    return y


def short_conv_mps_like(
    x_btd: torch.Tensor,
    weight_dw: torch.Tensor,
    bias_d: torch.Tensor | None,
    activation: bool = True,
    residual: bool = True,
    attention_mask: torch.Tensor | None = None,
):
    """
    ä½¿ç”¨è‡ªç ” MPS æ ¸æ¨¡æ‹Ÿ HF çš„ `ShortConvolution.forward`ï¼š
    - è¾“å…¥ [B, T, D]ï¼ŒæŒ‰ mask ç½®é›¶
    - èµ° [B, D, T] è·¯å¾„è°ƒç”¨ mps_ext.causal_conv1d_fwd
    - å¯é€‰ residual
    è¿”å› [B, T, D]
    """
    x_in = x_btd
    if attention_mask is not None:
        x_btd = x_btd * attention_mask.unsqueeze(-1)
    x_bdt = x_btd.movedim(-1, -2).contiguous()
    y_bdt = ccmps.causal_conv1d_fwd(
        x_bdt, weight_dw.contiguous(), bias_d.contiguous() if bias_d is not None else None, activation
    )
    y = y_bdt.movedim(-2, -1)
    if residual:
        y = x_in + y
    return y


def short_conv_mps_optimized(
    x_btd: torch.Tensor,
    weight_dw: torch.Tensor,
    bias_d: torch.Tensor | None,
    activation: bool = True,
    residual: bool = True,
    attention_mask: torch.Tensor | None = None,
):
    """
    ä½¿ç”¨ä¼˜åŒ–çš„ FUSED MPS å†…æ ¸ã€‚è¾“å…¥ (B, T, D)ã€‚é¿å…å¸ƒå±€æ›´æ”¹å¹¶èåˆæ‰€æœ‰æ“ä½œã€‚
    """
    x_contig = x_btd.contiguous()
    w_contig = weight_dw.contiguous()

    y = ccmps.short_conv_fused(
        x_contig, w_contig, bias_d, attention_mask, activation, residual
    )
    return y

def run_hf_like_canon_ac_bench():
    """
    æ¨¡æ‹Ÿ HF ä¸­ CanonA/C çš„ä½¿ç”¨ï¼š
    - è¾“å…¥ [B, T, D]
    - 2D attention_mask (B, T)
    - SiLU æ¿€æ´» + æ®‹å·®
    å¯¹æ¯” MPS ä¸ PyTorch å‚è€ƒå®ç°çš„æ€§èƒ½ä¸æ­£ç¡®æ€§ã€‚
    """
    device = torch.device("mps")
    torch.manual_seed(202)
    configs = [
        # (B, T, D, W)
        (2, 256, 768, 4),
        (4, 512, 1024, 4),
    ]

    print("\nğŸ§ª HF åœºæ™¯ (Optimized Fused)ï¼šCanonA/Cï¼ˆB,T,D + Fused Kernelï¼‰")
    print(f"{'Config':<28} {'MPS(ms)':<10} {'PyTorch(ms)':<12} {'Speedup':<10} {'MPS_StdDev(%)':<15} {'Correct':<8}")
    print("-" * 104)

    for bsz, seqlen, dim, width in configs:
        x_btd = torch.randn(bsz, seqlen, dim, device=device, dtype=DTYPE)
        weight_dw = torch.randn(dim, width, device=device, dtype=DTYPE)
        bias_d = torch.randn(dim, device=device, dtype=DTYPE)
        mask = generate_attention_mask(bsz, seqlen, device)

        cfg = f"B{bsz} T{seqlen} D{dim} W{width}"

        def run_ref():
            return short_conv_hf_reference(
                x_btd, weight_dw, bias_d, activation=True, residual=True, attention_mask=mask
            )

        def run_mps():
            return short_conv_mps_optimized(
                x_btd, weight_dw, bias_d, activation=True, residual=True, attention_mask=mask
            )

        t_ref, _ = bench_robust_stable(run_ref, warmup=800, iters=300, runs=150, desc=cfg)
        t_mps, std_mps = bench_robust_stable(run_mps, warmup=800, iters=300, runs=150, desc=cfg)

        y_ref = run_ref()
        y_mps = run_mps()
        max_diff = torch.max(torch.abs(y_ref - y_mps)).item()
        is_ok = max_diff < get_tolerance_for(DTYPE)

        sp = t_ref / t_mps
        std_pct = (std_mps / t_mps) * 100 if t_mps > 0 else 0
        print(
            f"{cfg:<28} {t_mps * 1000:<10.2f} {t_ref * 1000:<12.2f} {sp:<10.2f} {std_pct:<15.2f} {'âœ…' if is_ok else 'âŒ':<8}"
        )
        if not is_ok:
            print(f"  âš ï¸ æœ€å¤§å·®å¼‚: {max_diff:.6f}")


def run_hf_like_canon_b_bench():
    """
    æ¨¡æ‹Ÿ HF ä¸­ CanonB åœ¨ Attention é‡Œçš„ç”¨æ³•ï¼š
    - å°† Q, K, V åœ¨æœ€åç»´åº¦æ‹¼æ¥ï¼Œåš depthwise causal convï¼ˆSiLU + residualï¼‰
    - ä¸æ‰§è¡Œæ³¨æ„åŠ›ï¼Œä»…åŸºå‡†åŒ–è¿™ä¸€æ­¥
    """
    device = torch.device("mps")
    torch.manual_seed(203)
    configs = [
        # (B, T, num_heads, num_kv_heads, head_dim, W)
        (2, 256, 12, 4, 64, 4),  # Dq=768, Dk=256, Dv=256, Dtotal=1280
        (2, 512, 16, 8, 64, 4),  # Dq=1024, Dk=512, Dv=512, Dtotal=2048
    ]

    print("\nğŸ§ª HF åœºæ™¯ (Optimized Fused)ï¼šCanonBï¼ˆQKV è¿æ¥ + Fused Kernelï¼‰")
    print(f"{'Config':<40} {'MPS(ms)':<10} {'PyTorch(ms)':<12} {'Speedup':<10} {'MPS_StdDev(%)':<15} {'Correct':<8}")
    print("-" * 118)

    for bsz, seqlen, n_heads, n_kv, head_dim, width in configs:
        dq = n_heads * head_dim
        dk = n_kv * head_dim
        dv = n_kv * head_dim
        d_total = dq + dk + dv

        x_q = torch.randn(bsz, seqlen, dq, device=device, dtype=DTYPE)
        x_k = torch.randn(bsz, seqlen, dk, device=device, dtype=DTYPE)
        x_v = torch.randn(bsz, seqlen, dv, device=device, dtype=DTYPE)
        x_cat = torch.cat([x_q, x_k, x_v], dim=-1)

        weight_dw = torch.randn(d_total, width, device=device, dtype=DTYPE)
        bias_d = torch.randn(d_total, device=device, dtype=DTYPE)
        mask = generate_attention_mask(bsz, seqlen, device)

        cfg = f"B{bsz} T{seqlen} H{n_heads} KV{n_kv} hd{head_dim} W{width}"

        def run_ref():
            return short_conv_hf_reference(
                x_cat, weight_dw, bias_d, activation=True, residual=True, attention_mask=mask
            )

        def run_mps():
            return short_conv_mps_optimized(
                x_cat, weight_dw, bias_d, activation=True, residual=True, attention_mask=mask
            )

        t_ref, _ = bench_robust_stable(run_ref, warmup=800, iters=300, runs=150, desc=cfg)
        t_mps, std_mps = bench_robust_stable(run_mps, warmup=800, iters=300, runs=150, desc=cfg)

        y_ref = run_ref()
        y_mps = run_mps()
        max_diff = torch.max(torch.abs(y_ref - y_mps)).item()
        is_ok = max_diff < get_tolerance_for(DTYPE)

        sp = t_ref / t_mps
        std_pct = (std_mps / t_mps) * 100 if t_mps > 0 else 0
        print(
            f"{cfg:<40} {t_mps * 1000:<10.2f} {t_ref * 1000:<12.2f} {sp:<10.2f} {std_pct:<15.2f} {'âœ…' if is_ok else 'âŒ':<8}"
        )
        if not is_ok:
            print(f"  âš ï¸ æœ€å¤§å·®å¼‚: {max_diff:.6f}")


def run_hf_like_canon_d_bench():
    """
    æ¨¡æ‹Ÿ HF ä¸­ CanonD åœ¨ MLP é‡Œçš„ç”¨æ³•ï¼š
    - å°† gate_proj å’Œ up_proj çš„è¾“å‡ºåœ¨æœ€åç»´åº¦æ‹¼æ¥
    - åš depthwise causal convï¼ˆSiLU + residualï¼‰
    - ç„¶ååˆ†å‰²å›åŸæ¥çš„ç»´åº¦ç”¨äºåç»­çš„ down_proj
    """
    device = torch.device("mps")
    torch.manual_seed(204)
    configs = [
        # (B, T, hidden_size, intermediate_size, W) - æ¨¡æ‹Ÿ MLP é…ç½®
        (2, 256, 768, 2048, 4),   # å°æ¨¡å‹é…ç½®
        (2, 512, 1024, 4096, 4),  # ä¸­ç­‰æ¨¡å‹é…ç½®
    ]

    print("\nğŸ§ª HF åœºæ™¯ (Optimized Fused)ï¼šCanonDï¼ˆMLP Gate&Up è¿æ¥ + Fused Kernelï¼‰")
    print(f"{'Config':<35} {'MPS(ms)':<10} {'PyTorch(ms)':<12} {'Speedup':<10} {'MPS_StdDev(%)':<15} {'Correct':<8}")
    print("-" * 113)

    for bsz, seqlen, hidden_size, intermediate_size, width in configs:
        # æ¨¡æ‹Ÿ MLP ä¸­ gate_proj å’Œ up_proj çš„è¾“å‡º
        gate_output = torch.randn(bsz, seqlen, intermediate_size, device=device, dtype=DTYPE)
        up_output = torch.randn(bsz, seqlen, intermediate_size, device=device, dtype=DTYPE)
        
        # CanonD åº”ç”¨åœ¨è¿æ¥åçš„è¾“å‡ºä¸Š (intermediate_size * 2)
        x_cat = torch.cat([gate_output, up_output], dim=-1)
        
        weight_dw = torch.randn(intermediate_size * 2, width, device=device, dtype=DTYPE)
        bias_d = torch.randn(intermediate_size * 2, device=device, dtype=DTYPE)
        mask = generate_attention_mask(bsz, seqlen, device)

        cfg = f"B{bsz} T{seqlen} H{hidden_size} I{intermediate_size} W{width}"

        def run_ref():
            # CanonD å‚è€ƒå®ç°ï¼šå¯¹è¿æ¥åçš„å¼ é‡åšå·ç§¯ï¼Œç„¶ååˆ†å‰²
            conv_out = short_conv_hf_reference(
                x_cat, weight_dw, bias_d, activation=True, residual=True, attention_mask=mask
            )
            # åˆ†å‰²å› gate å’Œ up éƒ¨åˆ†
            gate_conv, up_conv = conv_out.chunk(2, dim=-1)
            return gate_conv, up_conv

        def run_mps():
            # CanonD MPS å®ç°
            conv_out = short_conv_mps_optimized(
                x_cat, weight_dw, bias_d, activation=True, residual=True, attention_mask=mask
            )
            # åˆ†å‰²å› gate å’Œ up éƒ¨åˆ†
            gate_conv, up_conv = conv_out.chunk(2, dim=-1)
            return gate_conv, up_conv

        t_ref, _ = bench_robust_stable(run_ref, warmup=800, iters=300, runs=150, desc=cfg)
        t_mps, std_mps = bench_robust_stable(run_mps, warmup=800, iters=300, runs=150, desc=cfg)

        gate_ref, up_ref = run_ref()
        gate_mps, up_mps = run_mps()
        
        # æ¯”è¾ƒä¸¤ä¸ªè¾“å‡ºçš„å·®å¼‚
        max_diff_gate = torch.max(torch.abs(gate_mps - gate_ref)).item()
        max_diff_up = torch.max(torch.abs(up_mps - up_ref)).item()
        max_diff = max(max_diff_gate, max_diff_up)
        is_ok = max_diff < get_tolerance_for(DTYPE)

        sp = t_ref / t_mps
        std_pct = (std_mps / t_mps) * 100 if t_mps > 0 else 0
        print(
            f"{cfg:<35} {t_mps * 1000:<10.2f} {t_ref * 1000:<12.2f} {sp:<10.2f} {std_pct:<15.2f} {'âœ…' if is_ok else 'âŒ':<8}"
        )
        if not is_ok:
            print(f"  âš ï¸ æœ€å¤§å·®å¼‚: gate={max_diff_gate:.6f}, up={max_diff_up:.6f}")


def main():
    print("ğŸš€ Causal Conv1D MPS æ€§èƒ½æµ‹è¯•")

    assert torch.backends.mps.is_available(), "MPS not available"


    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="CausalConv1D MPS Benchmarks")
    parser.add_argument("--only-hf", action="store_true", help="ä»…è¿è¡Œ HuggingFace é£æ ¼çš„ CanonA/Cã€CanonB ä¸ CanonD åŸºå‡†")
    parser.add_argument(
        "--dtype",
        type=str,
        default="bf16",
        choices=["bf16", "fp16", "fp32"],
        help="æµ‹è¯•æ•°æ®ç±»å‹ï¼ˆé»˜è®¤ bf16ï¼‰",
    )
    args = parser.parse_args()

    # æ ¹æ®å‚æ•°è®¾ç½®å…¨å±€ DTYPE ä¸å®¹å·®
    global DTYPE
    if args.dtype == "bf16":
        DTYPE = torch.bfloat16
    elif args.dtype == "fp16":
        DTYPE = torch.float16
    else:
        DTYPE = torch.float32

    if args.only_hf:
        run_hf_like_canon_ac_bench()
        run_hf_like_canon_b_bench()
        run_hf_like_canon_d_bench()
        return

    device = torch.device("mps")

    # æµ‹è¯•é…ç½®ï¼š(batch, dim, seqlen, width)
    test_configs = [
        (1, 64, 128, 4),  # å°è§„æ¨¡
        (2, 128, 256, 4),  # ä¸­ç­‰è§„æ¨¡
        (4, 256, 512, 4),  # å¤§è§„æ¨¡
        (1, 512, 1024, 4),  # è¶…å¤§è§„æ¨¡
        (8, 64, 128, 4),  # å¤§æ‰¹é‡
    ]

    print(
        f"{'Config':<20} {'MPS(ms)':<10} {'PyTorch(ms)':<12} {'Speedup':<10} {'MPS_StdDev(%)':<15} {'Correct':<8}"
    )
    print("-" * 80)

    for batch, dim, seqlen, width in test_configs:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        torch.manual_seed(42)
        x = torch.randn(batch, dim, seqlen, device=device, dtype=DTYPE)
        weight = torch.randn(dim, width, device=device, dtype=DTYPE)
        bias = torch.randn(dim, device=device, dtype=DTYPE)

        config_str = f"{batch}Ã—{dim}Ã—{seqlen}Ã—{width}"

        try:
            # MPS å®ç°
            def run_mps():
                return ccmps.causal_conv1d_fwd(
                    x.contiguous(), weight.contiguous(), bias.contiguous(), False
                )

            # PyTorch å‚è€ƒå®ç°
            def run_torch():
                return causal_conv1d_reference(x, weight, bias, False)

            # æ€§èƒ½æµ‹è¯•ï¼ˆä½¿ç”¨æ›´ç¨³å®šçš„æ–¹æ³•ï¼‰
            t_mps, std_mps = bench_robust_stable(
                run_mps, warmup=1000, iters=500, runs=210, desc=config_str
            )
            t_torch, _ = bench_robust_stable(
                run_torch, warmup=1000, iters=500, runs=210, desc=config_str
            )

            # æ­£ç¡®æ€§éªŒè¯
            result_mps = run_mps()
            result_torch = run_torch()
            max_diff = torch.max(torch.abs(result_mps - result_torch)).item()
            is_correct = max_diff < get_tolerance_for(DTYPE)

            speedup = t_torch / t_mps
            std_percent_mps = (std_mps / t_mps) * 100 if t_mps > 0 else 0
            print(
                f"{config_str:<20} {t_mps * 1000:<10.2f} {t_torch * 1000:<12.2f} {speedup:<10.2f} {std_percent_mps:<15.2f} {'âœ…' if is_correct else 'âŒ':<8}"
            )

            if not is_correct:
                print(f"  âš ï¸  æœ€å¤§å·®å¼‚: {max_diff:.6f}")

            # åœ¨ä¸åŒé…ç½®ä¹‹é—´ç¨å¾®ä¼‘æ¯ï¼Œç¼“è§£æ¸©åº¦å½±å“
            time.sleep(1)

        except Exception as e:
            print(
                f"{config_str:<20} {'ERROR':<10} {'ERROR':<12} {'ERROR':<10} {'ERROR':<15} {'âŒ':<8}"
            )
            print(f"  é”™è¯¯: {e}")

    # SiLU æ¿€æ´»å‡½æ•°æ€§èƒ½æµ‹è¯•
    print("\nğŸ”¥ SiLU æ¿€æ´»å‡½æ•°æ€§èƒ½æµ‹è¯•")
    print(
        f"{'Config':<20} {'MPS+SiLU(ms)':<14} {'PyTorch+SiLU(ms)':<17} {'Speedup':<10} {'MPS_StdDev(%)':<15}"
    )
    print("-" * 90)

    # é€‰æ‹©ä¸­ç­‰è§„æ¨¡æµ‹è¯•æ¿€æ´»å‡½æ•°
    batch, dim, seqlen, width = 2, 128, 256, 4
    config_str = f"{batch}Ã—{dim}Ã—{seqlen}Ã—{width}"

    torch.manual_seed(42)
    x = torch.randn(batch, dim, seqlen, device=device, dtype=DTYPE)
    weight = torch.randn(dim, width, device=device, dtype=DTYPE)
    bias = torch.randn(dim, device=device, dtype=DTYPE)

    try:

        def run_mps_silu():
            return ccmps.causal_conv1d_fwd(
                x.contiguous(), weight.contiguous(), bias.contiguous(), True
            )

        def run_torch_silu():
            return causal_conv1d_reference(x, weight, bias, True)

        t_mps_silu, std_mps_silu = bench_robust_stable(
            run_mps_silu, warmup=1000, iters=400, runs=210, desc=config_str
        )
        t_torch_silu, _ = bench_robust_stable(
            run_torch_silu, warmup=1000, iters=400, runs=210, desc=config_str
        )
        speedup_silu = t_torch_silu / t_mps_silu
        std_percent_mps_silu = (
            (std_mps_silu / t_mps_silu) * 100 if t_mps_silu > 0 else 0
        )

        print(
            f"{config_str:<20} {t_mps_silu * 1000:<14.2f} {t_torch_silu * 1000:<17.2f} {speedup_silu:<10.2f} {std_percent_mps_silu:<15.2f}"
        )

    except Exception as e:
        print(f"{config_str:<20} {'ERROR':<12} {'ERROR':<15} {'ERROR':<10}")
        print(f"  é”™è¯¯: {e}")

    print("\nğŸ“Š æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
    print(
        "ğŸ’¡ æç¤º: Speedup > 1.0 è¡¨ç¤º MPS å®ç°æ›´å¿«ã€‚StdDev(%) è¶Šå°ï¼Œè¡¨ç¤ºæµ‹è¯•ç»“æœè¶Šç¨³å®šã€‚"
    )

    # =====================
    # Canon åœºæ™¯ï¼ˆB, T, Dï¼‰åŸºå‡†
    # =====================
    print("\nğŸ§ª Canon ä½¿ç”¨åœºæ™¯åŸºå‡† (B,T,D æ¥å£)")
    print(
        f"{'Config':<24} {'MPS(ms)':<10} {'PyTorch(ms)':<12} {'Speedup':<10} {'MPS_StdDev(%)':<15} {'Correct':<8}"
    )
    print("-" * 96)

    device = torch.device("mps")
    torch.manual_seed(123)
    canon_configs = [
        # (B, T, D, W)
        (1, 128, 512, 4),
        (2, 256, 768, 4),
        (4, 512, 1024, 4),
    ]

    for bsz, seqlen, dim, width in canon_configs:
        x_btd = torch.randn(bsz, seqlen, dim, device=device, dtype=DTYPE)
        weight_dw = torch.randn(dim, width, device=device, dtype=DTYPE)
        bias_d = torch.randn(dim, device=device, dtype=DTYPE)

        cfg = f"B{bsz} T{seqlen} D{dim} W{width}"

        def run_mps_canon():
            # è½¬ä¸º [B, D, T] è·¯å¾„ä»¥å¤ç”¨æ ¸
            x_bdt = x_btd.movedim(-1, -2).contiguous()
            y_bdt = ccmps.causal_conv1d_fwd(
                x_bdt, weight_dw.contiguous(), bias_d.contiguous(), True
            )
            return y_bdt.movedim(-2, -1)

        def run_ref_canon():
            return canon_forward_reference(x_btd, weight_dw, bias_d, activation=True)

        t_ref, _ = bench_robust_stable(
            run_ref_canon, warmup=1000, iters=500, runs=210, desc=cfg
        )

        t_mps, std_mps = bench_robust_stable(
            run_mps_canon, warmup=1000, iters=500, runs=210, desc=cfg
        )
        y_mps = run_mps_canon()
        y_ref = run_ref_canon()
        max_diff = torch.max(torch.abs(y_mps - y_ref)).item()
        is_ok = max_diff < get_tolerance_for(DTYPE)

        sp = t_ref / t_mps
        std_pct = (std_mps / t_mps) * 100 if t_mps > 0 else 0
        print(
            f"{cfg:<24} {t_mps * 1000:<10.2f} {t_ref * 1000:<12.2f} {sp:<10.2f} {std_pct:<15.2f} {'âœ…' if is_ok else 'âŒ':<8}"
        )

        if not is_ok:
            print(f"  âš ï¸ æœ€å¤§å·®å¼‚: {max_diff:.6f}")

    # =====================
    # HF-like åœºæ™¯è¡¥å……ï¼šCanonA/Cã€CanonB ä¸ CanonD
    # =====================
    run_hf_like_canon_ac_bench()
    run_hf_like_canon_b_bench()
    run_hf_like_canon_d_bench()


if __name__ == "__main__":
    main()
